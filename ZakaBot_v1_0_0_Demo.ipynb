{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ZakaBot v1.0.0**\n",
        "\n",
        "**Last updated 11-05-2023 by Wissam M.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_x1GI7Fo8Y7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain==0.0.150 tiktoken transformers openai faiss-cpu"
      ],
      "metadata": {
        "id": "gk2J2sYYjTkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c2fe50-c72d-4016-f4fe-8547345b9796"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.4/648.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import pathlib\n",
        "from transformers import GPT2TokenizerFast\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "l-uszlwN641q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "E2Buv5Y0uFr8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir 'TrainingData'\n",
        "directory_path = '/content/TrainingData'\n",
        "directory_files = os.listdir(directory_path)"
      ],
      "metadata": {
        "id": "L1ztGwHH06-j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Manual step: drop the AI_Bootcamp_Syllabus.txt and AI_Certification_Syllabus.txt inside /content/TrainingData"
      ],
      "metadata": {
        "id": "oG8W_HI_oNtu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo\"\n",
        "min_match_error = 0.4"
      ],
      "metadata": {
        "id": "f8QuYaxrpXsO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Zaka_Bot:\n",
        "    vector_db = None\n",
        "    qa_chain = None\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"# Initializing Zaka_Bot class...\")\n",
        "        self.initialize_bot()\n",
        "        \n",
        "    def initialize_bot(self):\n",
        "        training_chunks = self.read_and_chunk_training_data()\n",
        "        self.initialize_vector_indices(training_chunks)\n",
        "        self.initialize_qa_chain()\n",
        "\n",
        "    def read_and_chunk_training_data(self) -> str:\n",
        "        training_data_dir_path = './TrainingData/'\n",
        "        training_data_files = os.listdir(training_data_dir_path)\n",
        "\n",
        "        training_text = ''\n",
        "        for file_name in training_data_files:\n",
        "            if pathlib.Path(file_name).suffix != '.txt':\n",
        "                continue\n",
        "\n",
        "            with open(training_data_dir_path + file_name, 'r') as f:\n",
        "                print('> Reading file:', file_name)\n",
        "                text = f.read()\n",
        "            \n",
        "            training_text += '\\n' + text\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size = 500,\n",
        "            chunk_overlap  = 24,\n",
        "            length_function = self.count_tokens)\n",
        "\n",
        "        chunks = text_splitter.create_documents([training_text])\n",
        "\n",
        "        print(\"> Training data chunked\")\n",
        "        return chunks\n",
        "\n",
        "    def initialize_qa_chain(self):\n",
        "        chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
        "        self.qa_chain = chain\n",
        "        print(\"> QA chain loaded\")\n",
        "\n",
        "    def initialize_vector_indices(self, chunks):\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "\n",
        "        vector_db = self.get_local_vector_indices(embeddings)\n",
        "\n",
        "        if not vector_db:\n",
        "            vector_db = FAISS.from_documents(chunks, embeddings)\n",
        "            self.save_vector_indices(vector_db)\n",
        "        else:\n",
        "            print('Existing local vector database detected')\n",
        "\n",
        "        self.vector_db = vector_db\n",
        "        print(\"> Vector database initialized\")\n",
        "\n",
        "    def get_local_vector_indices(self, embeddings):\n",
        "        try:\n",
        "            return FAISS.load_local(\"faiss_index\", embeddings)\n",
        "        except:\n",
        "            print('No local vector database detected')\n",
        "            return None\n",
        "\n",
        "    def save_vector_indices(self, vector_db):\n",
        "        vector_db.save_local(\"faiss_index\")\n",
        "        print(\"> Vector database saved locally\")\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "        return len(tokenizer.encode(text))\n",
        "\n",
        "    def assess_docs_and_score(self, docs_and_scores) -> bool:\n",
        "        for doc_and_score in docs_and_scores:\n",
        "            if doc_and_score[1] <= min_match_error:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def get_docs_from_docs_and_scores(self, docs_and_scores):\n",
        "        return [doc for doc, score in docs_and_scores]\n",
        "\n",
        "    def get_answer(self, question: str) -> str:\n",
        "        print('> Processing question...')\n",
        "        answer = ''\n",
        "\n",
        "        if self.vector_db is None:\n",
        "            raise Exception(\"Error: vector index not initialized\")\n",
        "        if self.qa_chain is None:\n",
        "            raise Exception(\"Error: qa_chain not initialized\")\n",
        "        \n",
        "        print(\"> Running similarity search\")\n",
        "        docs_and_scores = self.vector_db.similarity_search_with_score(question)\n",
        "\n",
        "        match_found = self.assess_docs_and_score(docs_and_scores)\n",
        "        print(\"> Found relevant information in vector corpus\" \n",
        "                      if match_found \n",
        "                      else f\"> No relevant information found in local vector corpus, querying {model}..\")\n",
        "\n",
        "        print(\"> Running QA chain...\")\n",
        "        docs = self.get_docs_from_docs_and_scores(docs_and_scores)\n",
        "        answer = self.qa_chain.run(input_documents=docs, question=question)\n",
        "        \n",
        "        answer = self.augment_answer(question, answer, match_found)\n",
        "\n",
        "        print(\"> Response received\")\n",
        "        return answer\n",
        "\n",
        "    def augment_answer(self, question: str, answer: str, match_found: bool) -> str:\n",
        "        identity_init = \"\"\"\n",
        "        You are 'Zaka Bot', a friendly chatbot and assistant who works for the company 'Zaka'\n",
        "        \"\"\"\n",
        "        idendity_message = {\"role\": \"system\", \"content\": identity_init}\n",
        "\n",
        "        rephrase_order = \"\"\"\n",
        "        Your next message will be your last message rephrased, without opening or trailing statements\n",
        "        \"\"\"\n",
        "\n",
        "        topic_adjustive = \"\"\"\n",
        "        At the end of the answer, ask the user if they have any questions about Zaka\n",
        "        \"\"\"\n",
        "\n",
        "        if match_found:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    idendity_message,\n",
        "                    {\"role\": \"user\", \"content\": question},\n",
        "                    {\"role\": \"assistant\", \"content\": answer},\n",
        "                    {\"role\": \"system\", \"content\": rephrase_order}\n",
        "                    ])\n",
        "            \n",
        "        else:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    idendity_message,\n",
        "                    {\"role\": \"user\", \"content\": question},\n",
        "                    {\"role\": \"assistant\", \"content\": topic_adjustive},\n",
        "                    ])\n",
        "\n",
        "        return response['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "Dbizh9hopE8r"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bot = Zaka_Bot()"
      ],
      "metadata": {
        "id": "iADY2CXNlNq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e24f06b-4c50-4073-8358-4025f141ef0e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Initializing Zaka_Bot class...\n",
            "> Reading file: AI_Bootcamp_Syllabus.txt\n",
            "> Reading file: AI_Certification_Syllabus.txt\n",
            "> Training data chunked\n",
            "No local vector database detected\n",
            "> Vector database saved locally\n",
            "> Vector database initialized\n",
            "> QA chain loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'Who is Ronaldo?'"
      ],
      "metadata": {
        "id": "eG1_LmEQsKgz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = bot.get_answer(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A709WEXvsTHQ",
        "outputId": "67e31b41-f390-4d2e-a287-a0c6ff6d0fd0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Processing question...\n",
            "> Running similarity search\n",
            "> No relevant information found in local vector corpus, querying gpt-3.5-turbo..\n",
            "> Running QA chain...\n",
            "> Response received\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "3YZz-tzhsWKZ",
        "outputId": "cd8f4aaa-39a8-44f4-d752-616e9b2b7e27"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ronaldo can refer to several people with the same name, but it is commonly associated with the famous Portuguese professional footballer Cristiano Ronaldo. He is widely considered one of the greatest football players of all time and has won numerous awards throughout his career. Is there anything related to Zaka that you would like to know more about?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}